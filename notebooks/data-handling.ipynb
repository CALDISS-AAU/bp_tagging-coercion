{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_f = os.path.join(\"..\", \"data\")\n",
    "#data_f = os.path.join(\"C:/\", \"Users\", \"kgk\", \"OneDrive - Aalborg Universitet\", \"CALDISS_projects\", \"2021-22_tagging-coercion\", \"data\")\n",
    "data_f = os.path.join(\"D:/\", \"OneDrive\", \"OneDrive - Aalborg Universitet\", \"CALDISS_projects\", \"2021-22_tagging-coercion\", \"data\")\n",
    "data_n = \"tc_anno-ents_comb.jsonl\"\n",
    "data_p = os.path.join(data_f, data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_p, 'r') as f:\n",
    "    data = [json.loads(jline) for jline in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating labels\n",
    "\n",
    "The code below isolates specific labels. A dataset for each label is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible span labels: CITY, NAME, ADDRESS\n",
    "span_labels = ['CITY', 'NAME', 'ADDRESS']\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for label in span_labels:\n",
    "    data_copy = copy.deepcopy(data)\n",
    "    for entry in data_copy:\n",
    "        entry['spans'] = [span for span in entry.get('spans') if span.get('label') == label]\n",
    "    \n",
    "    datasets[label] = data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {}\n",
    "for label in span_labels:\n",
    "    filename = f\"tc_{label}.json\"\n",
    "    filenames[label] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in span_labels:\n",
    "    data_out = os.path.join(data_f, filenames[label])\n",
    "    with open(data_out, 'w') as outfile:\n",
    "        json.dump(datasets[label], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [filename for filename in os.listdir(data_f) if \".jsonl\" in filename]\n",
    "filenames.remove(\"tc_pos-nouns.jsonl\") # Remove pos tagged data as texts are split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for filename in filenames:\n",
    "    data_p = os.path.join(data_f, filename)\n",
    "    \n",
    "    with open(data_p, 'r') as f:\n",
    "        data = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "        \n",
    "    all_data = all_data + data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_wanted = list(set([entry.get('ID') for entry in all_data]))\n",
    "ids_statements = list(set([entry.get('Id') for entry in all_data]))\n",
    "\n",
    "all_ids = list(filter(None, ids_wanted + ids_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entries = []\n",
    "ids_check = list(filter(None, ids_wanted + ids_statements))\n",
    "keep_keys = [\"ID\", \"Id\", \"set\", \"Navn_anmelder\", \"Navn_efterlyst\", \"Date\", \"text\"]\n",
    "\n",
    "for entry in all_data:\n",
    "    if (entry.get('ID') in ids_check) or (entry.get('Id') in ids_check):\n",
    "        entry_id = entry.get('ID')\n",
    "        if entry_id is None:\n",
    "            entry_id = entry.get('Id')\n",
    "        \n",
    "        entry_filtered = {keep_key: entry.get(keep_key) for keep_key in keep_keys}\n",
    "        \n",
    "        unique_entries.append(entry_filtered)\n",
    "        ids_check.remove(entry_id)\n",
    "        \n",
    "for entry in unique_entries:\n",
    "    if entry.get('Navn_efterlyst') is not None and entry.get('set') is None:\n",
    "        entry['set'] = \"Bortl√∏bningsannoncer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle\n",
    "\n",
    "random.shuffle(unique_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_n = \"tc_annotate-set-all-mixed_20220902.json\"\n",
    "out_p = os.path.join(data_f, out_n)\n",
    "\n",
    "# Export data\n",
    "with open(out_p, 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(unique_entries, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
